{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bacb88",
   "metadata": {},
   "source": [
    "<h2>üîç FreshCart Churn Prediction - Model Evaluation & Interpretation</h2>\n",
    " \n",
    "<h4>\n",
    "    <b>\n",
    "        Zero2End Machine Learning Bootcamp - Final Project\n",
    "    </b>\n",
    "</h4>\n",
    "\n",
    "<h3>\n",
    "    üìã Notebook Contents\n",
    "</h3>\n",
    "\n",
    "<ol>\n",
    "    <li>Load Final Model</li>\n",
    "    <li>Confusion Matrix & Error Analysis</li>\n",
    "    <li>Feature Importance Analysis</li>\n",
    "    <li>SHAP Values & Interpretation</li>\n",
    "    <li>Partial Dependence Plots</li>\n",
    "    <li>Business Metrics & Validation</li>\n",
    "    <li>Model Decision Analysis</li>\n",
    "</ol>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad20f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c550fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18870333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a593f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from config import PROCESSED_DATA_DIR, MODEL_DIR, RANDOM_STATE, BUSINESS_METRICS\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6bc43",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    1Ô∏è‚É£ Load Final Model and Data\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e2205f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading final model and data...\n"
     ]
    }
   ],
   "source": [
    "# Load final model\n",
    "print(\"üì¶ Loading final model and data...\")\n",
    "\n",
    "final_model = joblib.load(MODEL_DIR / 'final_model_optimized.pkl')\n",
    "\n",
    "with open(MODEL_DIR / 'final_metrics.json', 'r') as f:\n",
    "    final_metrics = json.load(f)\n",
    "\n",
    "with open(MODEL_DIR / 'feature_names.json', 'r') as f:\n",
    "    feature_names = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27347021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "final_features = pd.read_parquet(PROCESSED_DATA_DIR / 'final_features_advanced.parquet')\n",
    "\n",
    "X = final_features[feature_names].fillna(0)\n",
    "y = final_features['is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0154164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and data loaded\n",
      "   Test set: (41242, 53)\n",
      "   Model performance (F1): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Split (same as training)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model and data loaded\")\n",
    "print(f\"   Test set: {X_test.shape}\")\n",
    "print(f\"   Model performance (F1): {final_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = (final_model.predict(X_test) > 0.5).astype(int)\n",
    "y_pred_proba = final_model.predict(X_test)\n",
    "\n",
    "print(\"\\nüìä Prediction distribution:\")\n",
    "print(f\"   Predicted Churned: {y_pred.sum():,} ({y_pred.mean():.2%})\")\n",
    "print(f\"   Predicted Active:  {(y_pred == 0).sum():,} ({(y_pred == 0).mean():.2%})\")\n",
    "print(f\"   Actual Churned:    {y_test.sum():,} ({y_test.mean():.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a5dc8",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    2Ô∏è‚É£ Confusion Matrix & Error Analysis\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d794e513",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Confusion matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, \u001b[43my_pred\u001b[49m)\n\u001b[0;32m      4\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Confusion matrix (counts)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion matrix (counts)\n",
    "ax = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "            xticklabels=['Active', 'Churned'],\n",
    "            yticklabels=['Active', 'Churned'])\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Confusion matrix (percentages)\n",
    "ax = axes[1]\n",
    "cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Active', 'Churned'],\n",
    "            yticklabels=['Active', 'Churned'])\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_title('Confusion Matrix (Percentages)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/12_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
